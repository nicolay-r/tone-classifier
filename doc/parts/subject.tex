\newpage
\section{Обзор предметной области}
    % Объяснить, какие подходы к классификации рассматриваются в этой области.
    На сегодняшний день, большинство пользователей крупных социальных сетей,
    таких как Twitter, предпочитают высказывать свои отзывы и мнения о товарах
    и услугах в формате коротких сообщений. Возможность быстрого реагирования
    на сообщения со стороны компаний, предоставляемых эти услуги, возможно лишь
    в случае их автоматической обработки.

    Как известно, одним из направлений в решении задачи классификации является
    использование методов машинного обучения. Ввиду существенного роста объема
    доступной информации социальных сетей, такие проблемы как адаптация и
    обучение классификационных моделей становятся все менее значительными.
    В связи с этим, рассмотрим наиболее популярные методы машинного обучения \cite{svmAdvantages},
    которые находят свое применение к задачи тональной классификации сообщений.

    \subsection{Подходы к тональной классификации на основе методов машинного обучения}
        Основой работы рассматриваемых методов является представление исходных
        сообщений $m$ в формате вектора нормализованых слов $\{f_1, f_2, \ldots, f_n\}$.
        В качеcтве значений каждой из размерности вектора $\vec{m}$ можно сопоставить
        $n_{f_i}(m)$ -- число вхождений терма $f_i$ в рассматриваемое сообщение $m$.
        Такая модель является вариацией {\it Bag Of Words} \cite{svmAdvantages}.
        В результате, сообщение $m$ представляет собой вектор:
        \begin{equation}
            \vec{m} = (n_1(m), n_2(m), \ldots, n_k(m)) \nonumber
        \end{equation}

        \subsubsection{Метод <<Наивного Байеса>>}
        % На основе статьи
        Один из подходов к класификации сообщений заключается в определении класса $c^{*}$,
        к которому относится рассматриваемое сообщение $m$ на основе метрики
        {\it максимального правдоподобия}:
        \begin{equation}
            c^{*} = argmax_c \hspace{2pt} P(c|m) \nonumber
        \end{equation}

        В основе вычисления условной вероятности $P(c|m)$ лежит правило Байеса:
        \begin{equation}
            \label{eq:BayesRule}
            P(c|m) = \dfrac{P(c, m)}{P(m)} = \dfrac{P(c)\cdot P(m|c)}{P(m)}
        \end{equation}

        Классификатор, построенный на основе правила, представленного в
        формуле \ref{eq:BayesRule}, называется {\it NB}-классификатором.
        Для оценки условной вероятности в формуле \ref{eq:BayesRule}, предполагается
        независимость термов сообщения, и вычисляется следующим образом:
        \begin{equation}
            P_{NB} (c|m) = \dfrac{P(c)\cdot(\Pi_{i=1}^{n}P(f_i|c)^{n_i(m)})}{P(m)} \nonumber
        \end{equation}

        Несмотря на простоту реализации алгоритма, независимость термов $f_i$ сообщения
        является ограничением для достижения реального правдоподобия. Работа \cite{nbAdvantages}
        демонстрирует оптимальность метода Наивного Байеса для большинства задач
        классификации, в которых присутствуют признаки, позволяющие установить
        тесную связь с соответствующими классами. В тоже время, использование
        более сложных методов позволяет добиться лучших результатов.

        %\subsubsection{Метод <<Максимальной энтропии>>}
        % Из статьи
        \subsubsection{Метод <<Опорных векторов>>}
        % 9.1.2
        В отличие от метода <<Наивного Байеса>>, подход на основе рассматриваемого
        метода предполагает поиск гиперплоскости, разделяющей сообщения разных классов.
        Построение выполняется на этапе обучения модели. На этом этапе решается задача
        поиска нормали $\vec{w}$ к гиперплоскости, причем разбиение классов должно
        производиться с максимально возможным отступом.

        Для поиска нормали составляется {\it оптимизационная задача с граничными
        условиями}:
        \begin{equation}
            \label{eq:optimizationSVM}
            \vec{w} = \sum_{j=1}^{N} \alpha_i c_i \vec{m_i}, \hspace{2pt} \alpha_i \geq 0
        \end{equation}

        В уравнении \ref{eq:optimizationSVM}, коэффициент $c_i \in \{-1, 1\}$
        указывает на принадлежность сообщения $m_i$ соответствующему классу;
        $\alpha_i$ -- коээфициент решения задачи двойной оптимизации. Среди всех
        векторов $\vec{m}$, для которых выполнено условие $\alpha_i > 0$, называются <<опорными>>.
        Определения класса, к которому относится рассматриваемый документ, осуществляется
        на основе стороны гиперплоскости, на которую падает проекция вектора $\vec{m}$.

        В общем случае, классификатор построенный на рассматриваемом подходе,
        позволяет достичь лучших резульатов классификации если сравнивать с
        аналогом на основе метода <<Наивного Байеса>> \cite{svmCompareVsNB}.

        (Дополнить на основе аналогичного раздела из \cite{islr})

        % SVM with multiple classes
    \subsection{Признаки, используемые для классификации сообщений}
        % Лемматизация термов сообщения на основе метрик
        \subsubsection{Векторизация сообщений}
        % bag of words

        % tf-idf
        % tf-idf + вспомогательный словарь.
        \subsubsection{Вспомагательные признаки для сообщений}
        %

    \subsection{Способы построения тональных лексиконов}

    \subsection{Оценка качества классификационной модели}
    % Посмотреть обзоры из обзора соревнований.

        \subsubsection{Точность и полнота}
    Чтобы произвести оценку качества работы классификатора на некотором наборе
    сообщений, необходимо чтобы для этого набора существовали эталонные
    значения. Применительно к задаче тональной классификации, под значениями
    понимается класс, к которому необходимо отнести соответствующее сообщение.

    Таким образом, для каждого сообщения ответ может быть получен как со стороны
    классификатора, так и группой экспертов. Все возможные случаи ответов
    для фиксированного класса $A$ удобнее всего представить в таблице
    \ref{table:contingent}. Такое представление носит название
    {\it таблицы контингентности}.

    \begin{table}[ht]
        \centering
        \caption{Таблица контингентности для класса $A$}
        \label{table:contingent}
        \begin{tabular}{|l|}
            \hline
             \\ \hline
         \end{tabular}
     \end{table}

     На основе таблицы \ref{table:contingent} можно рассчитать следующие
     характеристики качества работы классификатора для соответсвующего класса:
    \begin{itemize}
        \item {\bf Полнота} -- число найденных сообщений, которые
            действительно принадлежат соответствующему классу относительно всех
            сообщений соответсвующего класса:

            \begin{equation}
                R_A = \dfrac{TP}{TP + FN} \hspace{20pt} \nonumber
            \end{equation}

        \item {\bf Точность} -- количество сообщений, которое
            классификатор правильно отнес к соответсвующему классу по отошению
            ко всему объему сообщений определенных системой в этот класс:

            \begin{equation}
                P_A = \dfrac{TP}{TP + FP} \hspace{20pt} \nonumber
            \end{equation}
    \end{itemize}

        На практике возникает необходимость в метрике, которая бы позволяла одновременно
    обе характеристики: точность и полноту. Для этого предусмотрена $F-мера$, которая
    в общем случае вычисляется по формуле:
    \begin{equation}
        \label{eq:fmeasureCommon}
        F(\beta) = \dfrac{(1+\beta^2) P \cdot R}{\beta^2 \cdot P + R}
    \end{equation}

    В случае, если $\beta = 1$, то формула \ref{eq:fmeasureCommon} преобразуется
    к виду:
    \begin{equation}
        \label{eq:fmeasure}
        F_1 = \dfrac{2 \cdot P R}{P + R}
    \end{equation}

    \subsubsection{$F_1-micro$ и $F_1-macro$ меры качества}
        В п. ... рассматривался способ вычисления полноты и точности применительно
    одного класса. В формуле \ref{eq:fmeasure} вычисление меры основывается
    на аналогичных параметрах, с той лишь разницей, что они вычислены для
    всех классов.

    Пусть имеются две коллекции данных $K_1$ и $K_2$, в которых каждое из сообщений
    принадлежит к одному из двух классов: положительному или отрицатльному.
    Тогда, на основе этих коллекций можно составить таблицы контингентности,
    аналогичные таблице \ref{table:contingent},и вычислить следующие параметры:
    \begin{itemize}
        \item {\it Микроусреднением полноты } -- называется величина,
        \item {\it Микроусреднением точности } -- называется величина,
    \end{itemize}
