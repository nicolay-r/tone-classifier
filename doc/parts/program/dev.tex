\subsection{Разработка тонального класификатора}
    % Архитектура проекта
    % Описать зависимости, а также что требуется реализовать.
    % Для решения задачи сентиментального анализа требуется:
    %   - классификатор (LibSVM)
    %   - модули обработки сообщений (Mystem + тональные префиксы + списки стоп слов)
    %
    % Выбор языка для реализации приложения
    %   - Python
    % хранения коллекций сообщений сети Twitter, а также вспомогательных данных:
    %   - База данных
    В качестве основы для разработки приложения был выбран язык {\it Python},
    ввиду следующих особенностей:
    \begin{enumerate}
        \item Отсутствие временных ограничений на обработку сообщений,
            классификацию;
        \item Возможность быстро подстраиваться под изменение архитектуры
            проекта и введения новых возможностей;
        \item Наличие библиотеки LibSVM\cite{svmClassifier} реализующей
            классификацию методом <<Опорных Векторов>>.
    \end{enumerate}

    Что касается вопроса хранения коллекций и вспомогательной информации
    для решения задачи классификации, для этих целей используется СУБД
    {\it PostgreSQL}. Такой выбор обусловлен удобным интерфейсом взаимодействия
    через сценарии языка {\it Python}, а также возможностью удобного консольного
    администрирования хранилища. Как и в случае с выбором языка для реализации
    задачи, на выбор хранилища не накладываются временные ограничения.

    Под разработкой проекта понимается реализация сценариев на языке Python,
    для выполнения следующих подздачач:
    %
    % Описать в формате потока, какие действия требуются чтобы произвести
    % классификацию с оценкой или без нее.
    %
    \begin{enumerate}
        \item Подготовка коллекций -- импорт XML данных в хранилище;
        \item Построение модели на основе обучающей коллекции;
        \item Применение модели к данным тестовой коллекции;
        \item Экспорт размеченных классификатором сообщений из хранилища в XML формат;
        \item Вычисление оценки качества работы модели.
    \end{enumerate}

    \subsubsection{Импорт/Экспорт сообщений}
    \label{sec:devImporting}
    % Описать используемые поля (про идентификаторы в частности)
    Для хранения исходных коллекций, данные формата \xml экспортировались в
    СУБД {\it PostrgreSQL}.
    В п. \ref{sec:programmingInnerFormat} рассматривается структура данных
    коллекции, с указанием на классы параметров.
    В зависимости от рассматриваемой задачи (см. п. \ref{sec:tonalityCompetition}),
    изменяется только набор дополнительных параметров, а основные остаются
    неизменными.
    Формат таблиц описывается с помощью {\it SQL} синтаксиса (см. п.
    \ref{sec:programmingInnerFormat}).
    В листинге \ref{lst:bankSQL} приведен пример сценария создания таблицы
    для сообщений в банковской сфере.

    % Листинги

    \lstset{language=sql}
    \lstinputlisting[caption="Сценарий создания таблицы для хранения тестовой
        коллекции сообщений банковской сферы",
        label={lst:bankSQL}]{parts/code/dev/bankSQL.sql}

    % Рассказать про использование psycopg
    % Модуль импорта сообщений x2pg_data.py
    Дополнительными полями в листинге \ref{lst:bankSQL} являются те,
    которые записаны в строках 6-13 и указывают на рассматриваемые компании.
    Сценарий создания таблицы для сферы ТКК отличается лишь набором дополнительных
    полей.

    Работа с таблицами {\it PostgreSQL} в языке {\it Python} производится с помощью
    библиотеки {\tt psycopg}.
    Для удобного чтения \xml файла используется библиотека {\tt libxml}.
    Преобразование \xml формата в хранилище рассматривается в листинге
    \ref{lst:importing}.

    \lstset{style=python}
    \lstinputlisting[caption="Импорт данных из формата XML в таблицу PostgreSQL",
        label={lst:importing}]{parts/code/dev/importing.py}

    % Модуль экспорта сообщений pg2x_data.py
    Экспорт сообщений из PostgreSQL в \xml формат (см. листинг
    \ref{lst:collectionMessageExample}) реализован с помощью библиотеки {\tt etree}
    для построения XML дерева.
    В листинге \ref{lst:exporting} приведен процесс построения дерева с помощью
    курсора, созданного для обхода таблицы СУБД.

    \lstset{style=python}
    \lstinputlisting[caption="Экспорт данных из таблицы PostgreSQL XML формат",
        label={lst:exporting}]{parts/code/dev/exporting.py}

    \subsubsection{Обработка сообщений сети Twitter}
    % Описать про модуль обработки.
    % Рассказать про применение тональных префиксов, удаление стоп слов.
    % А также, что все начинается с этапа лемматизации сообщения.
    % Что такое слово? -- последовательность символов, разделенная пробелами.
    Прежде чем перейти к описанию процесса преобразования, введем следующие
    понятия:
    \begin{itemize}
        \item {\bf Слово} --- это последовательность симоволов, которая ограничена
            пробельными символами, либо символами перевода строки.
        \item {\bf Терм} --- слова сообщения, к которым либо была применена
            лемматизация, либо применение лемматизации не требуется.
    \end{itemize}

    % рассказать про модуль в проекте. Описываем msg.py (можно просто предоставить
    % диаграмму UML для одного класса).

    Процесс обработки сообщений начинается с этапа разбиния всех слов сообщения
    на следующие классы (согласно п. \ref{sec:buildingMsgProcessing}):
    \begin{itemize}
        \item Имена пользователей сети \twitter --- термы с префиксом <<@>>;
        \item Хэштеги (от англ. {\it Hashtags}) --- термы с префиксом <<\#>>.
        \item Символы <<Ретвита>> --- термы со значением <<RT>>;
        \item Ссылки на ресурсы сети Интернет --- {\it URL\hspace{1pt}}-адреса;
        \item Основные слова --- множество слов, не вошедших ни в один из четырех
            выше перечисленных классов.
    \end{itemize}

    Код программы, отвечающий за выбор класса, к которому относится каждое из
    слов сообщения, представлен в листинге \ref{lst:classSelection}.

    \lstset{style=python}
    \lstinputlisting[caption="Определение класса для каждого из слов сообщения",
        label={lst:classSelection}]{parts/code/dev/classSelection.py}

    Все слова, не относящиеся ко множеству {\it основных слов}, не требуют
    дальнейшей лемматизации. Поэтому, ввиду выше введенных определений, слова
    этих множеств можно считать термами.

    % Лемматизация
    На следующем этапе обработки сообщения выполняется лемматизация основных слов.
    Результатом этого этапа должны стать леммы, представлюящие собой последовательность
    буквенных символов.
    Другими словами, леммы должны быть очищены от знаков препинания, эмотиконов,
    и т.п.

    Для преобразования слова в лемму, используется пакет {\it Mystem} компании
    {\it Yandex} \cite{mystem}.
    Такой пакет предоставляет одноименный класс {\it Mystem}, объект на основе
    которого используется для лемматизации текстовых сообщений.
    Лемматизация производится на основе словарей русского языка, которые
    добавляются в качестве компонента к пакету при первом его использовании.

    Для, того чтобы игнорировать знаки препинания, а также эмотиконы которые
    могут встречаться в конце слов, пакет предусматривает в качестве аргумента
    параметр {\it entire\_input} со значением {\it false}.
    Лемматизация основных слов сообщения приведена в листинге \ref{lst:lemmatization}.

    \lstset{style=python}
    \lstinputlisting[caption="Лемматизация основных слов сообщения",
        label={lst:lemmatization}]{parts/code/dev/lemmatize.py}

    % ЗДЕСЬ НЕ ОПИСАНО ПРО ИСПОЛЬЗОВАНИЕ СТОП СЛОВ И ТОНАЛЬНЫХ ПРЕФИКСОВ !!!

    % Получили список термов. Используем эти термы для обучения классификатора.
    После того как список термов получен, следующим шагом производится
    векторизация сообщения.

    \subsubsection{Использование LibSVM для классификации методом <<Опорных векторов>>}

    % Цели использования SVM классификатора
    % Здесь еще было бы неплохо сделать привязку к файлам.
    Сообщения в векторизованном виде используются как для обучения
    модели на основе классификатора SVM, так и для тестирования.
    Обучение или применение построенной модели к данным, зависит от типа коллекции,
    которая подверглась векторизации.
    Так, векторизация сообщений обучающей коллекции необходима для построения
    обучающей модели, а векторизация сообщений тестовой --- для разметки сообщений
    на основе уже построенной модели.

    % Какой формат входных данных (для обучения и для классификации).
    % Про обучение
    Рассмотрим формат представления коллекций, с которым рабтает пакет {\it LibSVM}.
    Чтобы создать классификационную модель, сообщения для обучения должны быть
    представлены в формате векторов, которые записываются следующим образом:
    \begin{center}
        \tt
        <label> <$index_1$>:<$value_1$> <$index_2$>:<$value_2$> ...
    \end{center}

    Рассмотрем параметры, входящие в шаблон:
    \begin{itemize}
        \item {\bf Метка} ({\it от англ. Label}) --- применительно к задаче
        тональной классификации, указывает на оценку которая была присвоена
        соотвествующему векторизованному сообщению;
        \item {\bf Индекс терма} --- уникальное числовое значение, используемое
        для идентифицирования соответствующего терма среди всех термов, встречающихся
        в коллекции;
        \item {\bf Значение} ({\it от англ. Value}) --- числовое значение,
        соответсвующее терму.
    \end{itemize}

    % Про применение
    По аналогии с форматом описания данных для обучения классификатора, вектора
    тестовой коллекции описываются аналогичным образом, с той лишь разницей, что
    в качестве {\it <<метки>>} может быть передано любое значение.
    Значение этого параметра не используется при классификации.
    В текущей реализации, этот парамет хранит идентификатор сообщения (поле
    {\it id}, см. п. \ref{sec:devImporting}).

    % Как производилась векторизация (модуль составления общего словаря voc).
    % indexer.py Модуль векторизации сообщений (зависит от хранения данных в бд)
    Поскольку индексы термов тестовой и обучающих коллекций должны быть
    синхронизованы, то индексация должна производится для объединенного
    множества термов каждой из коллекций.
    Модуль генерации индексов работает с коллекциями сообщений сети \twitter,
    представленных в формате таблиц хранилища. В Листинге \ref{lst:indexing}
    реализовано составление словаря на основе списка таблиц с коллекциями.

    \lstset{style=python}
    \lstinputlisting[caption="Получения словаря индексов для всех термов обучающей и тестовой коллекций",
        label={lst:indexing}]{parts/code/dev/indexing.py}

    % Что использовалось в качестве значения.
    Вычисление параметра {\it <<значение>>} производилось следующими способами:
    \begin{enumerate}
        \item На основе метрики tf-idf (см. формулу \ref{eq:tfidf};
        \item Искуственное повышение точности метрики tf-idf засчет информации о числе
            вхождении наиболее популярных термов в корпус из 1 миллиона документов.
    \end{enumerate}

    Во втором случае, информация числе документов, в которых содержится тот или
    иной терм, представлена в текстовом файле в формате списка:
    \begin{center}
        \it
        <<терм>> <<число документов, содержащих терм>>
    \end{center}

    Пусть $K$ --- основной корпус документов.
    Обозначим $E$ --- корпус, на основе которого была построена информация листинга
    \ref{lst:extendedFrequency}.
    Тогда, формула вычисления {\it инвертированной меры документов} будет выглядеть
    следующим образом:
    \begin{equation}
        idf(t, K \cup E) = log \Bigg[ \dfrac{|K \cup E|}{|\{d_i \in K \cup E | t_i \in d_i|\}|} \Bigg]
    \end{equation}

    % Запуск тестирования, и сохранение результатов
    % predict.py
    После того, как были подготовлены тестовые и обучающие коллекции к
    описанному выше формату, выполняются действия, за которые
    отвечает {\it модуль классификации} (реализация приведена в листинге
    \ref{lst:testing}):
    \begin{enumerate}
        \item Построение модели на основе обучающих данных (строки 4-5);
        \item Применение построенной модели к тестовой коллекции (строки 6-9).
    \end{enumerate}

    \lstset{style=python}
    \lstinputlisting[caption="Построение модели и ее применение к тестовым данным",
        label={lst:testing}]{parts/code/dev/testing.py}

    % Тестирование (модуль тестирования). Здесь не хватает бы общей архитектуры
    % создать, чтобы можно было в целом понимать как что взаимодействует.
    % Может имеет смысл потоком это отобразить.

    \subsubsection{Дополнительные признаки классификации}
    % Модуль добавления признаков в векторизацию сообщений.
    % Модуль features.py
    Рассмотрим реализацию дополнительных признаков п.
    \ref{sec:buildingAdditionalFeatures}.
    Признаки добавлялись в формате термов к сообщению.
    Такие термы представляли собой ключевые слова, префиксом которых является
    символ {\it <<\$>>}.

    Для обозначения признаков используются следующие ключевые слова (термы):
    \begin{itemize}
        \item {\tt \$emoticons} --- вычисление значения признака на основе
            множеств положительных и негативных эмотиконов.
            Для этого признака используется исходный текст сообщения
            (см. листинг \ref{lst:emoticons});
        \item {\tt \$capitals} --- признак, используемый для указания числа термов,
            записанных в верхнем регистре (см. листинг \ref{lst:capitals});

        \item {\tt \$signs} --- подсчет числа знаков препинания (см. листинг
            \ref{lst:signs}).
    \end{itemize}

    % Описание введения ключевых слов.
    \lstset{style=python}
    \lstinputlisting[caption="Вычисление значения признака на основе эмотиконов",
        label={lst:emoticons}]{parts/code/dev/emoticons.py}

    \lstset{style=python}
    \lstinputlisting[caption="Учет числа термов записанных в верхнем регистре",
        label={lst:capitals}]{parts/code/dev/capitals.py}

    \lstset{style=python}
    \lstinputlisting[caption="Подсчет числа знаков препинания",
        label={lst:signs}]{parts/code/dev/signs.py}

    % Списки спользуемых эмотиконов описать в разделе тестирования.

\subsection{Разработка вспомогательных инструментов}
% Выбор языка для реализации вспомогательных инструментов приложения
%   - Python
В роли вспомогательных инструментов выступают компоненты, которые в совокупности
решают задачу построения тонального лексикона на основе сообщений сети \twitter.
Реализация сценариев осуществляется посредством языка {\it Python}.

    \subsubsection{Балансировка исходных обучающих коллекций}
    % Описать с уклоном на атоматическую разметку сообщений на тональные классы.
    % (Как это было сделано в статьях)

    \subsubsection{Прием текстовых сообщений сети Twitter}

    \subsubsection{Создание лексиконов методом определения тональности словосочетаний}

